# PyTorch Distributed Language Model Training with Docker & DDP

This project demonstrates how to run **Distributed Data Parallel (DDP)** training using **PyTorch**, distributed across containers using **Docker Compose** and **overlay networks**. It simulates a realistic multi-node setup by running:

- 1 **master container**
- 1 **worker container** (on the same engine)
- 1 **remote worker container** (on another Docker engine joined via overlay network)

---

##  What is This Project About?

Training large-scale language models often requires distributing the workload across multiple machines or processes. PyTorch offers **Distributed Data Parallel (DDP)** to accelerate training and increase efficiency by parallelizing computation across multiple GPUs or CPUs.

This project trains a **Bigram Language Model** from scratch using text input (`input.txt`), and distributes the training across **multiple Docker containers** â€” even spanning different physical/virtual hosts â€” using DDP and Gloo backend.

---

##  What Problem is it Solving?

-  **Replicates a real-world distributed training environment** using containers across multiple nodes.
-  Leverages **PyTorch DDP** to speed up and synchronize training across distributed containers.
-  Uses **Docker Compose** + overlay networking for easy orchestration and scale-up/down without manually configuring nodes.
-  Helps simulate cluster-like behavior without setting up actual Kubernetes/Slurm/Horovod environments.

---

## Project Structure

```
pytorch-ddp-project/
â”‚
â”œâ”€â”€ container_a_master/         # Master node container
â”‚   â”œâ”€â”€ distributed_train.py    # DDP Training script (same in all)
â”‚   â”œâ”€â”€ docker-compose.yml      # Compose file for master container
â”‚   â””â”€â”€ Dockerfile              # Dockerfile for master
â”‚
â”œâ”€â”€ container_b_worker/         # Worker container on same Docker engine
â”‚   â”œâ”€â”€ distributed_train.py
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ container_c_worker/         # Worker container on another engine
â”‚   â”œâ”€â”€ distributed_train.py
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ input.txt                   # Training text file (shared across all)
â””â”€â”€ README.md                   # This file
```

> **Note:** The `distributed_train.py` is shared across all containers. Only the `RANK` environment variable differs across master and worker containers.

---

## Key Components Used

- **PyTorch DDP (Gloo backend):** For model parallelism and synchronization
- **Docker Compose:** For orchestration of local containers
- **Overlay Network:** To connect containers across different Docker engines
- **Custom Language Model:** A transformer-style bigram model built from scratch
- **Text File Input:** Raw `.txt` used to generate vocabulary and tokenize

---

## How to Run This Project

> Make sure Docker is installed on both engines and they are joined to the same Docker Swarm or overlay network (`pytorch-dist-overlay-net`).

---

###  On the Main Engine

#### Create the overlay network:
```bash
docker network create --driver overlay --attachable pytorch-dist-overlay-net
```

#### Start Master + Local Worker:

```bash
cd container_a_master
docker-compose up --build -d
```

```bash
cd ../container_b_worker
docker-compose up --build -d
```

---

###  On the Second Engine (Another Machine)

> Ensure itâ€™s connected to the same Docker Swarm or overlay network.

```bash
cd container_c_worker
docker-compose up --build -d
```

---

###  Check Logs

You can follow logs of each container:

```bash
docker logs -f <container_name>
```

---

##  Configuration

Each containerâ€™s `docker-compose.yml` sets:

- `RANK`: the unique ID for that node (0 for master, 1/2 for workers)
- `WORLD_SIZE`: total number of processes (3 in this setup)
- `MASTER_ADDR`: IP or container name of the master container
- `MASTER_PORT`: Port used for distributed communication (8080)
- `GLOO_SOCKET_IFNAME`: Interface for Gloo (set to `eth0` or your overlay interface)

---

##  Example Training Output

Once launched, you'll see output from each rank like:

```
Rank 0: Starting training for 5000 iterations...
step 0: train loss 3.25, val loss 3.10
step 500: train loss 2.80, val loss 2.65
...
```

---

##  Model Output

After training:

- The model is saved to `bigram_language_model.pth` (only by Rank 0 / master).
- Text is generated using the trained model and printed to logs.

---

##  Requirements

- Docker & Docker Compose
- Python 3.8+
- PyTorch (installed inside container)
- Shared overlay network
- Sufficient memory (1GB+ per container)

---

##  Additional Tips

- You can add more workers by duplicating the structure of `container_b_worker`, updating rank/world size.
- You can switch to `nccl` backend if using CUDA and compatible GPUs.
- Tune `batch_size`, `max_iters`, and model architecture in `distributed_train.py`.

---

##  License

This project is open-sourced under the [MIT License](https://opensource.org/licenses/MIT).

---

##  Author

Built with ðŸ’» and â˜• by **[Adarsh Srivastav](https://github.com/sdeadarsh)**

---

Let me know if you want to:
- Add a badge/shield (Docker, PyTorch, MIT License)
- Include demo logs or screenshots
- Convert this to a template for public use

Happy deploying! 